<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>databases on Akash Girme's Blog</title><link>https://blog.akashgirme.com/tags/databases/</link><description>Recent content in databases on Akash Girme's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 22 Apr 2021 00:00:00 +0530</lastBuildDate><atom:link href="https://blog.akashgirme.com/tags/databases/index.xml" rel="self" type="application/rss+xml"/><item><title>Working with PostgreSQL</title><link>https://blog.akashgirme.com/blog/working-with-postgresql/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0530</pubDate><guid>https://blog.akashgirme.com/blog/working-with-postgresql/</guid><description>This post is in the context of the large, data heavy PostgreSQL instances that store historical transactional data and reports, the databases that power Console and its large scale number crunching and reporting. It talks about how we self-host, tune, and manage all our DB instances on bare EC2 instances. For high availability and backups, we use simple failover replicas and for backups, AWS disk snapshots.
The Console DBs store hundreds of billions of rows of different kinds of financial and transactional data, currently close to 20 TB, across four sharded nodes.</description></item></channel></rss>